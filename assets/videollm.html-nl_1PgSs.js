import{_ as t}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as o,d as n,o as a}from"./app-BAhkXHOE.js";const r={};function i(d,e){return a(),o("div",null,e[0]||(e[0]=[n('<p>Awesome-Multimodal-Large-Language-Models<br><a href="https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models" target="_blank" rel="noopener noreferrer">https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models</a></p><p>VideoLLaMA 2: Advancing Spatial-Temporal Modeling and Audio Understanding in Video-LLMs</p><p><a href="https://github.com/DAMO-NLP-SG/VideoLLaMA2" target="_blank" rel="noopener noreferrer">https://github.com/DAMO-NLP-SG/VideoLLaMA2</a></p><p>AskVideos-VideoCLIP<br><a href="https://github.com/AskYoutubeAI/AskVideos-VideoCLIP" target="_blank" rel="noopener noreferrer">https://github.com/AskYoutubeAI/AskVideos-VideoCLIP</a></p><p>InternVideo2: Scaling Video Foundation Models for Multimodal Video Understanding<br><a href="https://github.com/OpenGVLab/InternVideo" target="_blank" rel="noopener noreferrer">https://github.com/OpenGVLab/InternVideo</a></p><p>INTERNVIDEO2: SCALING VIDEO FOUNDATION MODELS FOR MULTIMODAL VIDEO UNDERSTANDING<br><a href="https://github.com/OpenGVLab/InternVideo2" target="_blank" rel="noopener noreferrer">https://github.com/OpenGVLab/InternVideo2</a></p><p>InternVid: A Large-scale Video-Text Dataset for Multimodal Understanding and Generation<br><a href="https://github.com/OpenGVLab/InternVideo/tree/main/Data/InternVid" target="_blank" rel="noopener noreferrer">https://github.com/OpenGVLab/InternVideo/tree/main/Data/InternVid</a></p><p>ViCLIP: a video-text representation learning model trained on InternVid<br><a href="https://github.com/OpenGVLab/InternVideo/tree/main/InternVideo1/Pretrain/ViCLIP" target="_blank" rel="noopener noreferrer">https://github.com/OpenGVLab/InternVideo/tree/main/InternVideo1/Pretrain/ViCLIP</a></p>',8)]))}const g=t(r,[["render",i]]),s=JSON.parse(`{"path":"/discover/uncover/videollm.html","title":"","lang":"en-US","frontmatter":{"description":"Awesome-Multimodal-Large-Language-Models https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models VideoLLaMA 2: Advancing Spatial-Temporal Modeling and Audio Understa...","head":[["meta",{"property":"og:url","content":"https://wendongwish.github.io/wendongwish/discover/uncover/videollm.html"}],["meta",{"property":"og:site_name","content":"Hongyi's Blog"}],["meta",{"property":"og:description","content":"Awesome-Multimodal-Large-Language-Models https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models VideoLLaMA 2: Advancing Spatial-Temporal Modeling and Audio Understa..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"property":"og:updated_time","content":"2025-03-25T07:24:51.000Z"}],["meta",{"property":"article:modified_time","content":"2025-03-25T07:24:51.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2025-03-25T07:24:51.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Hongyi Wang\\",\\"url\\":\\"https://wendongwish.github.io/wendongwish\\"}]}"]]},"git":{"createdTime":1742887491000,"updatedTime":1742887491000,"contributors":[{"name":"HongyiWang","username":"HongyiWang","email":"why_6267@163.com","commits":1,"url":"https://github.com/HongyiWang"}]},"readingTime":{"minutes":0.27,"words":80},"filePathRelative":"discover/uncover/videollm.md","localizedDate":"March 25, 2025","excerpt":"<p>Awesome-Multimodal-Large-Language-Models<br>\\n<a href=\\"https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models</a></p>\\n<p>VideoLLaMA 2: Advancing Spatial-Temporal Modeling and Audio Understanding in Video-LLMs</p>","autoDesc":true}`);export{g as comp,s as data};
